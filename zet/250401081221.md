# ML/Attention

Softmax = (e^x / Î£0->n e^x_n)

Softmax is used as a way to normalize vectors for neural networks. It has
better handling of extreme values.


```
tags
[ml](../tags/ml.md)

```

